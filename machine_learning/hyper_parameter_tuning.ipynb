{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmem(total=17137168384, available=10992021504, percent=35.9, used=6145146880, free=10992021504)\n"
     ]
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import psutil  \n",
    "from sqlalchemy import create_engine\n",
    "from config import mypass\n",
    "\n",
    "\n",
    "# Notebook options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#Data Paths\n",
    "flights_ml_data = 'resources/flights_cleaned.csv'\n",
    "airlines_dict = 'resources/airline_dict.json'\n",
    "airports_dict = 'resources/airport_dict.json'\n",
    "\n",
    "print(psutil.virtual_memory()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 14\n"
     ]
    }
   ],
   "source": [
    "# Open dictionaries for encoding\n",
    "\n",
    "with open(\"resources/airport_dict.json\") as f:\n",
    "    airport_dict = json.load(f)\n",
    "\n",
    "    \n",
    "with open(\"resources/airline_dict.json\") as f:\n",
    "    airline_dict = json.load(f)\n",
    "    \n",
    "print(len(airport_dict), len(airline_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node 1: 90,  node2: 61,  mse: 390.13755495458986\n",
      "node 1: 90,  node2: 90,  mse: 388.9505550966797\n",
      "node 1: 90,  node2: 123,  mse: 389.5697041079102\n",
      "node 1: 90,  node2: 246,  mse: 389.51734975634764\n",
      "node 1: 123,  node2: 61,  mse: 389.9432755024414\n",
      "node 1: 123,  node2: 90,  mse: 389.6478765073242\n",
      "node 1: 123,  node2: 123,  mse: 389.0565438261719\n",
      "node 1: 123,  node2: 246,  mse: 389.62237896435545\n",
      "node 1: 246,  node2: 61,  mse: 388.89868199462893\n",
      "node 1: 246,  node2: 90,  mse: 389.03143599316405\n",
      "node 1: 246,  node2: 123,  mse: 390.33747130566405\n",
      "node 1: 246,  node2: 246,  mse: 388.4041834975586\n"
     ]
    }
   ],
   "source": [
    "# number of Nodes testing\n",
    "\n",
    "\n",
    "# Define the model\n",
    "number_input_features = 123\n",
    "hidden_nodes_layer1 = [90, number_input_features, number_input_features*2, ]\n",
    "hidden_nodes_layer2 = [int(number_input_features/2), 90, number_input_features, number_input_features*2]\n",
    "epochs = [3, 5, 10]\n",
    "\n",
    "\n",
    "\n",
    "for node1 in hidden_nodes_layer1:\n",
    "    for node2 in hidden_nodes_layer2:\n",
    "        \n",
    "        nn = tf.keras.models.Sequential()\n",
    "        \n",
    "        # First hidden layer\n",
    "        nn.add(tf.keras.layers.Dense(units=node1, input_dim=number_input_features, activation=\"relu\"))\n",
    "        # Second hidden layer\n",
    "        nn.add(tf.keras.layers.Dense(units=node2, activation=\"relu\"))\n",
    "        # Output layer\n",
    "        nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "        # Compile the Sequential model together and customize metrics\n",
    "        nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "        \n",
    "        # Load data from resource file in chunks\n",
    "        i = 1\n",
    "        chunksize = 10** 6\n",
    "        enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "        # Create a StandardScaler instance\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "\n",
    "        # Proccess data, and train model in chuncks, saving/loading the model weights for each chunk\n",
    "\n",
    "        for chunk in pd.read_csv(flights_ml_data, chunksize=chunksize):\n",
    "\n",
    "            # Use uppercase for CSV chunks, lowercase for database chunks    \n",
    "\n",
    "            y = chunk.DEPARTURE_DELAY\n",
    "            X = chunk.drop(columns=['DEPARTURE_DELAY', 'FLIGHT_NUMBER', 'TAIL_NUMBER', 'ELAPSED_TIME', 'AIR_TIME', 'WHEELS_ON', 'TAXI_IN', 'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'ARRIVAL_DELAY', 'AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY', 'Unnamed: 0'])\n",
    "\n",
    "            # map string values to int using airport dict and airline dict\n",
    "            for airport, id_num in airport_dict.items():\n",
    "                X['ORIGIN_AIRPORT'].replace(airport, id_num, inplace=True)\n",
    "                X['DESTINATION_AIRPORT'].replace(airport, id_num, inplace=True)\n",
    "\n",
    "            for airline, id_num in airline_dict.items():\n",
    "                X['AIRLINE'].replace(airline, id_num, inplace=True)\n",
    "\n",
    "            # Encode categorical values, fit transform, get variable names, merge and remove old columns\n",
    "            categorical_data = ['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'AIRLINE']\n",
    "\n",
    "            encode_df = pd.DataFrame(enc.fit_transform(X[categorical_data]))\n",
    "            encode_df.columns = enc.get_feature_names(categorical_data)\n",
    "\n",
    "            # Merge encoded df and original df, remove pre-encoded columns\n",
    "            # Resets the index, now have to drop added index column\n",
    "            X = X.reset_index()  \n",
    "            X.drop(columns=['index'], inplace=True)\n",
    "            encode_df = encode_df.reset_index()\n",
    "            encode_df.drop(columns=['index'], inplace=True)\n",
    "            X = X.merge(encode_df, left_index=True, right_index=True)\n",
    "            X.drop(columns=categorical_data, inplace=True)\n",
    "\n",
    "            # Split into training and testing data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, stratify=None)\n",
    "\n",
    "            \n",
    "            # Fit the StandardScaler\n",
    "            X_scaler = scaler.fit(X_train)\n",
    "            # Scale the data\n",
    "            X_train_scaled = X_scaler.transform(X_train)\n",
    "            X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            # Train the model\n",
    "            fit_model = nn.fit(X_train_scaled, y_train, epochs=5, verbose = 0)\n",
    "\n",
    "            # Evaluate the model using the test data\n",
    "            model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "\n",
    "            \n",
    "            print(f'node 1: {node1},  node2: {node2},  mse: {model_loss}')\n",
    "            \n",
    "            i+=1\n",
    "            \n",
    "            # only test on first chunk otherwise this will take an entire day\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first fn: relu,  second fn: relu,  mes: 389.44113661132815\n",
      "first fn: relu,  second fn: exponential,  mes: 393.4043212265625\n",
      "first fn: relu,  second fn: linear,  mes: 395.02030680664063\n",
      "first fn: relu,  second fn: sigmoid,  mes: 390.0631171748047\n",
      "first fn: relu,  second fn: tanh,  mes: 393.8799197163086\n",
      "first fn: exponential,  second fn: relu,  mes: 394.9877534604492\n",
      "first fn: exponential,  second fn: exponential,  mes: nan\n",
      "first fn: exponential,  second fn: linear,  mes: 396.9606965751953\n",
      "first fn: exponential,  second fn: sigmoid,  mes: 409.03629706884766\n",
      "first fn: exponential,  second fn: tanh,  mes: 411.6163557392578\n",
      "first fn: linear,  second fn: relu,  mes: 392.8749214116211\n",
      "first fn: linear,  second fn: exponential,  mes: 409.30919636376956\n",
      "first fn: linear,  second fn: linear,  mes: 403.09626523876955\n",
      "first fn: linear,  second fn: sigmoid,  mes: 397.1821991333008\n",
      "first fn: linear,  second fn: tanh,  mes: 398.61958030615233\n",
      "first fn: sigmoid,  second fn: relu,  mes: 393.01995237695314\n",
      "first fn: sigmoid,  second fn: exponential,  mes: 388.1370960317383\n",
      "first fn: sigmoid,  second fn: linear,  mes: 392.7183186010742\n",
      "first fn: sigmoid,  second fn: sigmoid,  mes: 392.8962692646484\n",
      "first fn: sigmoid,  second fn: tanh,  mes: 395.51457996289065\n",
      "first fn: tanh,  second fn: relu,  mes: 388.0235129428711\n",
      "first fn: tanh,  second fn: exponential,  mes: 399.60956958740235\n",
      "first fn: tanh,  second fn: linear,  mes: 393.7182873696289\n",
      "first fn: tanh,  second fn: sigmoid,  mes: 389.0882199384766\n",
      "first fn: tanh,  second fn: tanh,  mes: 389.8669205234375\n",
      "\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Activation function testing\n",
    "\n",
    "# Define the model\n",
    "number_input_features = 123\n",
    "hidden_nodes_layer1 = 123\n",
    "hidden_nodes_layer2 = 123\n",
    "activation_fns =['relu', 'exponential', 'linear', 'sigmoid', 'tanh']\n",
    "epochs = [3, 5, 10]\n",
    "\n",
    "\n",
    "for first in activation_fns:\n",
    "    for second in activation_fns:\n",
    "\n",
    "        nn = tf.keras.models.Sequential()\n",
    "\n",
    "        # First hidden layer\n",
    "        nn.add(tf.keras.layers.Dense(units=node1, input_dim=number_input_features, activation=first))\n",
    "        # Second hidden layer\n",
    "        nn.add(tf.keras.layers.Dense(units=node2, activation=second))\n",
    "        # Output layer\n",
    "        nn.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "\n",
    "        # Compile the Sequential model together and customize metrics\n",
    "        nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "        # Load data from resource file in chunks\n",
    "        i = 1\n",
    "        chunksize = 10** 6\n",
    "        enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "        # Create a StandardScaler instance\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "\n",
    "        # Proccess data, and train model in chuncks, saving/loading the model weights for each chunk\n",
    "\n",
    "        for chunk in pd.read_csv(flights_ml_data, chunksize=chunksize):\n",
    "\n",
    "            # Use uppercase for CSV chunks, lowercase for database chunks    \n",
    "\n",
    "            y = chunk.DEPARTURE_DELAY\n",
    "            X = chunk.drop(columns=['DEPARTURE_DELAY', 'FLIGHT_NUMBER', 'TAIL_NUMBER', 'ELAPSED_TIME', 'AIR_TIME', 'WHEELS_ON', 'TAXI_IN', 'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'ARRIVAL_DELAY', 'AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY', 'Unnamed: 0'])\n",
    "\n",
    "            # map string values to int using airport dict and airline dict\n",
    "            for airport, id_num in airport_dict.items():\n",
    "                X['ORIGIN_AIRPORT'].replace(airport, id_num, inplace=True)\n",
    "                X['DESTINATION_AIRPORT'].replace(airport, id_num, inplace=True)\n",
    "\n",
    "            for airline, id_num in airline_dict.items():\n",
    "                X['AIRLINE'].replace(airline, id_num, inplace=True)\n",
    "\n",
    "            # Encode categorical values, fit transform, get variable names, merge and remove old columns\n",
    "            categorical_data = ['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'AIRLINE']\n",
    "\n",
    "            encode_df = pd.DataFrame(enc.fit_transform(X[categorical_data]))\n",
    "            encode_df.columns = enc.get_feature_names(categorical_data)\n",
    "\n",
    "            # Merge encoded df and original df, remove pre-encoded columns\n",
    "            # Resets the index, now have to drop added index column\n",
    "            X = X.reset_index()  \n",
    "            X.drop(columns=['index'], inplace=True)\n",
    "            encode_df = encode_df.reset_index()\n",
    "            encode_df.drop(columns=['index'], inplace=True)\n",
    "            X = X.merge(encode_df, left_index=True, right_index=True)\n",
    "            X.drop(columns=categorical_data, inplace=True)\n",
    "\n",
    "            # Split into training and testing data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, stratify=None)\n",
    "\n",
    "\n",
    "            # Fit the StandardScaler\n",
    "            X_scaler = scaler.fit(X_train)\n",
    "            # Scale the data\n",
    "            X_train_scaled = X_scaler.transform(X_train)\n",
    "            X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            # Train the model\n",
    "            fit_model = nn.fit(X_train_scaled, y_train, epochs=5, verbose = 0)\n",
    "\n",
    "            # Evaluate the model using the test data\n",
    "            model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "\n",
    "            print(f'first fn: {first},  second fn: {second},  mes: {model_loss}')\n",
    "\n",
    "            i += 1\n",
    "            # only test on first chunk otherwise this will take an entire day\n",
    "            break\n",
    "\n",
    "print(\"\\nfinished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output fn: relu,  mes: 391.30573986621096, loss: 0.04151200130581856\n",
      "output fn: exponential,  mes: 393.5145560385742, loss: 0.03800800070166588\n",
      "output fn: linear,  mes: 388.9225799580078, loss: 0.03823599964380264\n",
      "output fn: sigmoid,  mes: 453.692526144043, loss: 0.03418000042438507\n",
      "output fn: tanh,  mes: 453.938224, loss: 0.03214799985289574\n"
     ]
    }
   ],
   "source": [
    "# output activation function testing\n",
    "\n",
    "# Define the model\n",
    "number_input_features = 123\n",
    "hidden_nodes_layer1 = 123\n",
    "hidden_nodes_layer2 = 123\n",
    "activation_fns =['relu', 'exponential', 'linear', 'sigmoid', 'tanh']\n",
    "epochs = [3, 5, 10]\n",
    "\n",
    "\n",
    "for output in activation_fns:\n",
    "\n",
    "\n",
    "    nn = tf.keras.models.Sequential()\n",
    "\n",
    "    # First hidden layer\n",
    "    nn.add(tf.keras.layers.Dense(units=node1, input_dim=number_input_features, activation='relu'))\n",
    "    # Second hidden layer\n",
    "    nn.add(tf.keras.layers.Dense(units=node2, activation='relu'))\n",
    "    # Output layer\n",
    "    nn.add(tf.keras.layers.Dense(units=1, activation=output))\n",
    "\n",
    "    # Compile the Sequential model together and customize metrics\n",
    "    nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    # Load data from resource file in chunks\n",
    "    i = 1\n",
    "    chunksize = 10** 6\n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    # Create a StandardScaler instance\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "\n",
    "    # Proccess data, and train model in chuncks, saving/loading the model weights for each chunk\n",
    "\n",
    "    for chunk in pd.read_csv(flights_ml_data, chunksize=chunksize):\n",
    "\n",
    "        # Use uppercase for CSV chunks, lowercase for database chunks    \n",
    "\n",
    "        y = chunk.DEPARTURE_DELAY\n",
    "        X = chunk.drop(columns=['DEPARTURE_DELAY', 'FLIGHT_NUMBER', 'TAIL_NUMBER', 'ELAPSED_TIME', 'AIR_TIME', 'WHEELS_ON', 'TAXI_IN', 'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'ARRIVAL_DELAY', 'AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY', 'Unnamed: 0'])\n",
    "\n",
    "        # map string values to int using airport dict and airline dict\n",
    "        for airport, id_num in airport_dict.items():\n",
    "            X['ORIGIN_AIRPORT'].replace(airport, id_num, inplace=True)\n",
    "            X['DESTINATION_AIRPORT'].replace(airport, id_num, inplace=True)\n",
    "\n",
    "        for airline, id_num in airline_dict.items():\n",
    "            X['AIRLINE'].replace(airline, id_num, inplace=True)\n",
    "\n",
    "        # Encode categorical values, fit transform, get variable names, merge and remove old columns\n",
    "        categorical_data = ['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'AIRLINE']\n",
    "\n",
    "        encode_df = pd.DataFrame(enc.fit_transform(X[categorical_data]))\n",
    "        encode_df.columns = enc.get_feature_names(categorical_data)\n",
    "\n",
    "        # Merge encoded df and original df, remove pre-encoded columns\n",
    "        # Resets the index, now have to drop added index column\n",
    "        X = X.reset_index()  \n",
    "        X.drop(columns=['index'], inplace=True)\n",
    "        encode_df = encode_df.reset_index()\n",
    "        encode_df.drop(columns=['index'], inplace=True)\n",
    "        X = X.merge(encode_df, left_index=True, right_index=True)\n",
    "        X.drop(columns=categorical_data, inplace=True)\n",
    "\n",
    "        # Split into training and testing data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, stratify=None)\n",
    "\n",
    "\n",
    "        # Fit the StandardScaler\n",
    "        X_scaler = scaler.fit(X_train)\n",
    "        # Scale the data\n",
    "        X_train_scaled = X_scaler.transform(X_train)\n",
    "        X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        # Train the model\n",
    "        fit_model = nn.fit(X_train_scaled, y_train, epochs=5, verbose = 0)\n",
    "\n",
    "        # Evaluate the model using the test data\n",
    "        model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "\n",
    "        print(f'output fn: {output},  mes: {model_loss}, accuracy: {model_accuracy}')\n",
    "\n",
    "        i += 1\n",
    "        # only test on first chunk otherwise this will take an entire day\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 3,  mse: 392.5247253364258, accuracy: 0.03680000081658363\n",
      "epochs: 5,  mse: 389.0853060766602, accuracy: 0.03821200132369995\n",
      "epochs: 10,  mse: 388.5565029580078, accuracy: 0.03883599862456322\n",
      "epochs: 15,  mse: 387.64053872558594, accuracy: 0.037675999104976654\n",
      "epochs: 100,  mse: 399.87182602685544, accuracy: 0.03789199888706207\n"
     ]
    }
   ],
   "source": [
    "# number of epochs testing\n",
    "\n",
    "\n",
    "# Define the model\n",
    "number_input_features = 123\n",
    "hidden_nodes_layer1 = 123\n",
    "hidden_nodes_layer2 = 123\n",
    "\n",
    "epochs = [3, 5, 10, 15, 100]\n",
    "\n",
    "\n",
    "for epoch in epochs:\n",
    "\n",
    "    nn = tf.keras.models.Sequential()\n",
    "\n",
    "    # First hidden layer\n",
    "    nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "    # Second hidden layer\n",
    "    nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "    # Output layer\n",
    "    nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "    # Compile the Sequential model together and customize metrics\n",
    "    nn.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    # Load data from resource file in chunks\n",
    "    i = 1\n",
    "    chunksize = 10** 6\n",
    "    enc = OneHotEncoder(sparse=False, categories='auto')\n",
    "    # Create a StandardScaler instance\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "\n",
    "    # Proccess data, and train model in chuncks, saving/loading the model weights for each chunk\n",
    "\n",
    "    for chunk in pd.read_csv(flights_ml_data, chunksize=chunksize):\n",
    "\n",
    "        # Use uppercase for CSV chunks, lowercase for database chunks    \n",
    "\n",
    "        y = chunk.DEPARTURE_DELAY\n",
    "        X = chunk.drop(columns=['DEPARTURE_DELAY', 'FLIGHT_NUMBER', 'TAIL_NUMBER', 'ELAPSED_TIME', 'AIR_TIME', 'WHEELS_ON', 'TAXI_IN', 'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'ARRIVAL_DELAY', 'AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY', 'Unnamed: 0'])\n",
    "\n",
    "        # map string values to int using airport dict and airline dict\n",
    "        for airport, id_num in airport_dict.items():\n",
    "            X['ORIGIN_AIRPORT'].replace(airport, id_num, inplace=True)\n",
    "            X['DESTINATION_AIRPORT'].replace(airport, id_num, inplace=True)\n",
    "\n",
    "        for airline, id_num in airline_dict.items():\n",
    "            X['AIRLINE'].replace(airline, id_num, inplace=True)\n",
    "\n",
    "        # Encode categorical values, fit transform, get variable names, merge and remove old columns\n",
    "        categorical_data = ['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'AIRLINE']\n",
    "\n",
    "        encode_df = pd.DataFrame(enc.fit_transform(X[categorical_data]))\n",
    "        encode_df.columns = enc.get_feature_names(categorical_data)\n",
    "\n",
    "        # Merge encoded df and original df, remove pre-encoded columns\n",
    "        # Resets the index, now have to drop added index column\n",
    "        X = X.reset_index()  \n",
    "        X.drop(columns=['index'], inplace=True)\n",
    "        encode_df = encode_df.reset_index()\n",
    "        encode_df.drop(columns=['index'], inplace=True)\n",
    "        X = X.merge(encode_df, left_index=True, right_index=True)\n",
    "        X.drop(columns=categorical_data, inplace=True)\n",
    "\n",
    "        # Split into training and testing data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, stratify=None)\n",
    "\n",
    "\n",
    "        # Fit the StandardScaler\n",
    "        X_scaler = scaler.fit(X_train)\n",
    "        # Scale the data\n",
    "        X_train_scaled = X_scaler.transform(X_train)\n",
    "        X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "        # Train the model\n",
    "        fit_model = nn.fit(X_train_scaled, y_train, epochs=epoch, verbose = 0)\n",
    "\n",
    "        # Evaluate the model using the test data\n",
    "        model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=0)\n",
    "\n",
    "        print(f'epochs: {epoch},  mse: {model_loss}, accuracy: {model_accuracy}')\n",
    "\n",
    "        i+=1\n",
    "\n",
    "\n",
    "        # only test on first chunk otherwise this will take an entire day\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
